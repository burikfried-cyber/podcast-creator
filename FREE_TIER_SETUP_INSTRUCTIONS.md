# ğŸ†“ FREE Tier Setup Instructions

## âœ… What I've Done So Far

### **1. Created Content Services:**
- âœ… `WikipediaService` - Fetches real facts (FREE)
- âœ… `LocationService` - Gets location data (FREE)
- âœ… `LLMService` - Generates content with Ollama (FREE)

### **2. Updated Narrative Engine:**
- âœ… Fetches real Wikipedia content
- âœ… Fetches location details
- âœ… Uses LLM for hooks and conclusions
- âœ… Integrates real facts into narratives

### **3. Installing Dependencies:**
- â³ Running: `pip install wikipedia geopy ollama TTS pydub soundfile`

---

## ğŸ”§ What You Need to Do

### **Step 1: Install Ollama** (5 minutes)

**Download & Install:**
1. Go to: https://ollama.ai/download
2. Download Windows installer
3. Run the installer
4. Open a new terminal

**Pull the model:**
```bash
ollama pull llama3
```

**Verify:**
```bash
ollama list
# Should show: llama3
```

**Test it:**
```bash
ollama run llama3 "Tell me about Paris in 2 sentences"
```

You should see a response! This means Ollama is working.

---

### **Step 2: Verify Python Dependencies**

After the pip install completes, verify:

```bash
cd backend
python -c "import wikipedia; print('Wikipedia OK')"
python -c "import geopy; print('Geopy OK')"
python -c "import ollama; print('Ollama OK')"
```

All should print "OK".

---

### **Step 3: Create Storage Directory**

```bash
mkdir -p backend/storage/podcasts
```

---

### **Step 4: Update Environment Variables**

**File:** `backend/.env`

Add these lines:
```bash
# LLM Configuration
USE_OPENAI=false
OLLAMA_MODEL=llama3

# TTS Configuration  
USE_ELEVENLABS=false

# Storage
AUDIO_STORAGE_PATH=./storage/podcasts
```

---

## ğŸš€ What Happens Next

### **When You Generate a Podcast:**

**1. Content Fetching (5-10 seconds):**
- âœ… Fetches Wikipedia article about location
- âœ… Extracts interesting facts
- âœ… Gets location coordinates and details
- âœ… Logs: "real_content_fetched"

**2. Narrative Generation (20-30 seconds):**
- âœ… Ollama generates engaging hook
- âœ… Creates story structure with real facts
- âœ… Generates compelling conclusion
- âœ… Logs: "narrative_construction_complete"

**3. Script Assembly (5-10 seconds):**
- âœ… Assembles complete script
- âœ… Adds transitions
- âœ… Optimizes for speech
- âœ… Logs: "script_assembly_complete"

**4. Quality Check (2-3 seconds):**
- âœ… Validates content quality
- âœ… Checks engagement score
- âœ… Logs: "quality_check_complete"

**Total Time: 30-50 seconds** (much more realistic!)

---

## ğŸ“Š What You'll Get

### **Script Quality:**
- âœ… Real facts from Wikipedia
- âœ… Actual location information
- âœ… AI-generated narrative (Ollama)
- âœ… Engaging storytelling
- âœ… 1500-2000 words

### **Example Output:**
```
Title: Discover Paris, France

Hook: "What if I told you that Paris, the City of Light, 
holds secrets that have shaped Western civilization for 
over two millennia?"

Content: [Real facts from Wikipedia about Paris's history, 
culture, landmarks, and significance]

Conclusion: "From its Roman origins to its modern-day 
splendor, Paris continues to inspire millions..."
```

---

## ğŸµ Audio Generation (Coming Next)

After we test content generation, I'll add:

### **FREE Option: Coqui TTS**
- âœ… Open source
- âœ… Runs locally
- âœ… Decent quality
- âœ… $0 cost

### **Implementation:**
```python
from TTS.api import TTS

tts = TTS("tts_models/en/ljspeech/tacotron2-DDC")
tts.tts_to_file(text=script, file_path="podcast.wav")
```

---

## ğŸ§ª Testing Plan

### **Step 1: Test Content Fetching**
```bash
# Start backend
cd backend
uvicorn app.main:app --reload

# Watch logs for:
# - "fetching_real_content"
# - "real_content_fetched"
# - "narrative_construction_complete"
```

### **Step 2: Generate Test Podcast**
1. Go to http://localhost:5173
2. Login
3. Click "Generate"
4. Enter "Tokyo, Japan"
5. Click "Generate Podcast"
6. **Watch the logs!**

### **Expected Logs:**
```
INFO: fetching_real_content location="Tokyo, Japan"
INFO: real_content_fetched title="Tokyo" facts_count=10
INFO: generating_with_ollama model="llama3"
INFO: ollama_generation_complete length=2500
INFO: narrative_construction_complete
INFO: script_assembly_complete
INFO: quality_check_complete
INFO: podcast_generation_complete
```

### **Expected Duration:**
- â±ï¸ 30-50 seconds (realistic!)
- Not <1 second like before

---

## âš ï¸ Troubleshooting

### **If Ollama fails:**
```bash
# Check if Ollama is running
ollama list

# If not, start it
ollama serve

# Then pull model again
ollama pull llama3
```

### **If Wikipedia fails:**
```python
# Test manually
python
>>> import wikipedia
>>> page = wikipedia.page("Paris")
>>> print(page.summary[:200])
```

### **If generation is still too fast:**
- Check logs for "generating_with_ollama"
- If missing, Ollama isn't being called
- Verify `USE_OPENAI=false` in .env

---

## ğŸ“ˆ Quality Comparison

### **Before (Placeholder Content):**
```
Hook: "What if I told you that Paris, France holds a mystery..."
Content: [Generic template text]
Duration: <1 second
Quality: â­â­ (placeholder)
```

### **After (Real Content + Ollama):**
```
Hook: "In the heart of Europe lies a city that has witnessed 
revolutions, inspired artists, and defined romance for centuries..."
Content: [Real Wikipedia facts + AI narrative]
Duration: 30-50 seconds
Quality: â­â­â­â­ (good, engaging)
```

---

## ğŸ¯ Success Criteria

**You'll know it's working when:**
1. âœ… Generation takes 30-50 seconds (not <1 second)
2. âœ… Script contains real facts about the location
3. âœ… Content is engaging and well-written
4. âœ… Logs show "generating_with_ollama"
5. âœ… Script is 1500-2000 words

---

## ğŸš€ Next Steps After Testing

1. **Test content generation** with FREE tier
2. **Add Coqui TTS** for audio (also FREE)
3. **Test end-to-end** flow
4. **Optionally upgrade** to paid services if desired

---

## ğŸ’¡ Pro Tips

### **Speed Up Ollama:**
- Use smaller model: `ollama pull phi` (faster, less quality)
- Use GPU if available (automatic)
- Reduce max_tokens in LLM service

### **Improve Quality:**
- Use larger model: `ollama pull llama3:70b` (slower, better quality)
- Adjust temperature in LLM service
- Add more Wikipedia facts

### **Save Money:**
- Stay on FREE tier for testing
- Only upgrade to paid when ready for production
- Mix & match: FREE content + PAID audio

---

## âœ… Ready to Test!

**Once Ollama is installed:**
1. Verify it's working: `ollama run llama3 "test"`
2. Start backend: `uvicorn app.main:app --reload`
3. Generate a podcast
4. **Watch the magic happen!** âœ¨

**Let me know when Ollama is installed and I'll help you test!** ğŸš€
