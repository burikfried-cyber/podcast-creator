# ğŸ¯ ROOT CAUSE FOUND - Perplexity Model Name Issue

## ğŸ” The Problem

**Scripts contained template text instead of real AI-generated content.**

---

## ğŸ“Š Evidence from Logs

```
ERROR | app.services.content.llm_service | 
{
  "error": "Error code: 400 - {
    'error': {
      'message': \"Invalid model 'llama-3.1-sonar-small-128k-online'. 
                  Permitted models can be found in the documentation.\",
      'type': 'invalid_model',
      'code': 400
    }
  }",
  "event": "hook_generation_failed"
}
```

---

## ğŸ¯ Root Cause

**Perplexity deprecated the old model names!**

### **Old Models (DEPRECATED):**
- âŒ `llama-3.1-sonar-small-128k-online`
- âŒ `llama-3.1-sonar-large-128k-online`
- âŒ `llama-3.1-sonar-small-128k-chat`
- âŒ `llama-3.1-sonar-large-128k-chat`

### **New Models (CURRENT):**
- âœ… `sonar` - Basic model with web search
- âœ… `sonar-pro` - Better performance
- âœ… `sonar-reasoning` - Advanced reasoning
- âœ… `sonar-reasoning-pro` - Best reasoning
- âœ… `sonar-deep-research` - Deep research

---

## ğŸ”§ The Fix

### **File:** `backend/app/services/content/llm_service.py`

```python
# BEFORE (WRONG - DEPRECATED MODEL)
self.model = "llama-3.1-sonar-large-128k-online"

# AFTER (CORRECT - CURRENT MODEL)
self.model = "sonar"  # Basic model with web search
```

---

## âœ… What This Fixes

### **Before:**
1. LLM tries to call Perplexity with old model name
2. API returns 400 error (invalid model)
3. Error handler catches exception
4. Falls back to template text
5. **Result:** Template scripts like "Let's continue..."

### **After:**
1. LLM calls Perplexity with correct model name
2. API returns real AI-generated content
3. Hook and conclusion are generated with context
4. **Result:** Real, engaging content about the location!

---

## ğŸ§ª Testing

### **Restart Backend:**
```bash
cd backend
# Kill the current process (Ctrl+C)
uvicorn app.main:app --reload
```

### **Expected Log:**
```
âœ… LLM initialized with Perplexity | model: sonar
```

### **Generate New Podcast:**
1. Location: "Tokyo, Japan"
2. Wait for completion
3. Check script content

### **Expected Script:**
```
Tokyo, the vibrant capital of Japan, seamlessly blends ancient traditions 
with cutting-edge technology. From the serene temples of Asakusa to the 
neon-lit streets of Shibuya, this megacity offers endless discoveries...

[Real facts from Perplexity!]
```

---

## ğŸ“Š Why This Happened

1. **Perplexity updated their API** (2025)
2. **Deprecated old `llama-3.1-sonar-*` models**
3. **Introduced simpler names**: `sonar`, `sonar-pro`, etc.
4. **Our code used old model names**
5. **API rejected requests with 400 error**
6. **Fallback to templates triggered**

---

## ğŸ¯ Verification Checklist

After restarting backend:

- [ ] Backend starts without errors
- [ ] Log shows: `LLM initialized with Perplexity | model: sonar`
- [ ] Generate new podcast
- [ ] Check backend logs for:
  - [ ] `llm_generate_hook_called`
  - [ ] `generating_with_perplexity`
  - [ ] `llm_generation_complete` (no errors!)
- [ ] Script contains real facts (not templates)
- [ ] No "Let's continue..." text
- [ ] Engaging, context-aware narrative

---

## ğŸš€ Model Upgrade Options

If you want better quality, you can upgrade to:

```python
# Good quality (current)
self.model = "sonar"

# Better quality (recommended for production)
self.model = "sonar-pro"

# Best reasoning
self.model = "sonar-reasoning-pro"
```

**Note:** Better models may have different pricing. Check Perplexity docs.

---

## ğŸ“ Summary

**Problem:** Invalid model name â†’ API error â†’ Template fallback

**Solution:** Update to current model name (`sonar`)

**Result:** Real AI-generated content! ğŸ‰

---

## ğŸŠ Ready to Test!

**Restart your backend and generate a new podcast!**

The script should now contain real, engaging facts about the location instead of template text.

All previous fixes (singleton, query optimization, logging) are still in place and working correctly. This was the final missing piece! ğŸš€
